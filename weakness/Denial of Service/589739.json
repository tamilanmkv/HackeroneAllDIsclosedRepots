{"id":589739,"global_id":"Z2lkOi8vaGFja2Vyb25lL1JlcG9ydC81ODk3Mzk=","url":"https://hackerone.com/reports/589739","title":"Multiple HTTP/2 DOS Issues","state":"Closed","substate":"resolved","severity_rating":"high","readable_substate":"Resolved","created_at":"2019-05-24T20:53:30.231Z","submitted_at":"2019-05-24T20:53:30.231Z","is_member_of_team?":false,"reporter":{"disabled":false,"username":"jasnell","url":"/jasnell","profile_picture_urls":{"small":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/3afcb5c896247e7ee8ada31b1c1eb8657e22241f911093acfe4ec7e97a3a959a"},"is_me?":false,"cleared":false,"hackerone_triager":false,"hacker_mediation":false},"team":{"id":22984,"url":"https://hackerone.com/nodejs","handle":"nodejs","profile_picture_urls":{"small":"https://profile-photos.hackerone-user-content.com/variants/000/022/984/e600648ace4a8553247bce967d461a030aa81d49_original.png/3afcb5c896247e7ee8ada31b1c1eb8657e22241f911093acfe4ec7e97a3a959a","medium":"https://profile-photos.hackerone-user-content.com/variants/000/022/984/e600648ace4a8553247bce967d461a030aa81d49_original.png/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"permissions":[],"submission_state":"open","default_currency":"usd","awards_miles":false,"offers_bounties":true,"state":"public_mode","only_cleared_hackers":false,"profile":{"name":"Node.js","twitter_handle":"nodejs","website":"https://nodejs.org","about":"The Node.js JavaScript Runtime"}},"has_bounty?":false,"in_validation?":false,"rejected_anc_report_that_can_be_sent_back_to_anc_triagers?":false,"can_view_team":true,"can_view_report":true,"is_external_bug":false,"is_published":false,"is_participant":false,"stage":4,"public":true,"visibility":"full","cve_ids":[],"singular_disclosure_disabled":false,"disclosed_at":"2019-08-16T23:40:09.482Z","bug_reporter_agreed_on_going_public_at":"2019-08-16T23:40:09.421Z","team_member_agreed_on_going_public_at":"2019-08-16T15:28:56.804Z","comments_closed?":false,"facebook_team?":false,"team_private?":false,"vulnerability_information":"A security researcher has conducted a broad survey of HTTP/2 implementations to investigate common Denial of Service attack vectors. The Node.js implementation has been found to be subject to a number of these issues. (On the plus side, we're not the only ones! ;-) ...)\n\nThis work is still under embargo and has not yet been disclosed. \n\nSpecifically:\n\n* Data Dribble Attack: \"This program will request 1MB of data from a specified resource. It will request this same resource over 100 streams (so, 100MB total). It manipulates window sizes and stream priority to force the server to queue the data in 1-byte chunks.\"\n\n* Ping Flood (nginx variant):  \"Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others) has a 10K-message limit on the number of control messages it will queue. Sending a controlled number of messages may enable an attacker to force the server to hold 10K messages in memory...\"\n\n* Resource Loop: \"(actually, it should be called “Priority Shuffling”): This program continually shuffles the priority of streams in a way which causes substantial churn to the priority tree. Node.js [is] particularly impacted.\"\n\n* Reset Flood: \"This opens a number of streams and sends an invalid request over each stream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may queue the RSTs internally until they run out of memory.\"\n\n* O-Length Headers Leak: \"This sends a stream of headers with a 0-length header name and 0-length header value. [Node.js] allocates memory for these headers and keeps the allocation alive until the session dies. Because the names and values are 0 bytes long, the cumulative length never exceeds the header size limit.\"\n\n* Internal Data Buffering: \"This opens the HTTP/2 window so the server can send without constraint; however, it leaves the TCP window closed so the server cannot actually write (many of) the bytes on the wire. Then, the client sends a stream of requests for a large response object which the target queues internally. This appears to work to create a long-ish standing queue in node.js\"\n\nEach is a distinct issue that will need to be looked at individually. I've edited the descriptions to remove references to vulnerabilities in other HTTP/2 implementations that have not yet been disclosed.\n\n---\n\nAdditional details from the report:\n\n```\n“Data Dribble” on node.js: node.js seems to queue the data internally. For a 1MB output file\nrequested 100 times in parallel fast enough that node.js is constantly processing input,\nnode.js’s RSS rises by 808MB and then falls by 120MB (for an aggregate rise of 688MB).\n(Actually, it looks like the numbers vary a bit across tests, but I think the end result is “a lot”.)\nHowever, node.js does not have the excess CPU utilization which Nginx exhibits. If you\ninstead delay the sends considerably so that node.js has time to try to send in the meantime, it\nlooks like node.js will kill off the session before the input queue grows more than a few\nhundred MB.\n\n“Internal Data Buffering” on node.js: For a 1MB output file requested 100 times in parallel\n(but sent with 24 requests per SSL frame), node.js behaves in an interesting way. It appears to\nbuffer some, but not all, data internally. It seems to continue reading (and processing requests\nand queueing data to satisfy those requests) for as many streams as it can until it can’t read\nany more. Once it can’t read anymore, it appears to try to write and realize the writing is\nblocked. At that point, it seems to switch to reading frames from the wire and queuing the\nrequests internally (without processing them). (All of this is conjecture and is based on what\nI’ve observed rather than a detailed analysis of the code.) So, if you pack the 100 requests\ninto a single SSL frame, node.js’s RSS increases by approximately 246MB. Or, if you send\n585 requests in a single SSL frame, node.js’s RSS increases by approximately 1,296MB. For\nreasons that are not entirely clear to me, if you send 100K requests each on three different\nconnections (approximately 2.8MB of request data per connection, node.js will run out of\nmemory and crash. The other interesting thing that happens is on the session ending. When\nthe session ends, it looks like node.js temporarily starts reading everything which is left in the\ninput queue, tries to process the requests, and store the request output in memory. So,\nsending 100,000 requests (approximately 2.8MB of request data) and then closing the\nconnection can make node.js temporarily use 12GB of RSS.\n\nResource Loop on node.js: Over the loopback interface, node.js can handle roughly ~10 Mb/s\nbefore the assigned thread uses 100% of its CPU core (on an m5.24xlarge). RSS rose from\n50MB at the start of the test to 236MB by the end of test (~3 minutes). RSS rose another\n156MB when a second stream was added. With two streams, serving of content to another\n(non-attacking) connection was severely impacted.\n\nZero-length Headers on node.js: With truly 0-length headers (i.e. the payload is 0 bytes), the\nserver will accept and process an unlimited number; however, they don’t seem to create a\nstanding queue on the server side. The processing overhead is much lighter than the\n“Resource Loop” test. (Roughly 25 Mb/s only produces a 75% CPU load on the server.) With\n0-length headers which are Huffman encoded into 1-byte or greater headers, the server input\nfor that socket (and only that socket) seems to get blocked for ~ 2 minutes, until the\nconnection is killed off. It appears that the server will hold the connection open even if the\nclient goes away. That behavior allows a different kind of DoS attack (exhaust server file\ndescriptors or kernel receive buffers).\n\nReset Flood on node.js: The server queue grows without an obvious bound until the\nconnection dies or the server runs out of memory and dies. After the connection ends, the\nserver is unresponsive while GC runs\n```\n\n## Impact\n\nMultiple denial of service vectors.","vulnerability_information_html":"\u003cp\u003eA security researcher has conducted a broad survey of HTTP/2 implementations to investigate common Denial of Service attack vectors. The Node.js implementation has been found to be subject to a number of these issues. (On the plus side, we\u0026#39;re not the only ones! ;-) ...)\u003c/p\u003e\n\n\u003cp\u003eThis work is still under embargo and has not yet been disclosed. \u003c/p\u003e\n\n\u003cp\u003eSpecifically:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eData Dribble Attack: \u0026quot;This program will request 1MB of data from a specified resource. It will request this same resource over 100 streams (so, 100MB total). It manipulates window sizes and stream priority to force the server to queue the data in 1-byte chunks.\u0026quot;\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ePing Flood (nginx variant):  \u0026quot;Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others) has a 10K-message limit on the number of control messages it will queue. Sending a controlled number of messages may enable an attacker to force the server to hold 10K messages in memory...\u0026quot;\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eResource Loop: \u0026quot;(actually, it should be called “Priority Shuffling”): This program continually shuffles the priority of streams in a way which causes substantial churn to the priority tree. Node.js [is] particularly impacted.\u0026quot;\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eReset Flood: \u0026quot;This opens a number of streams and sends an invalid request over each stream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may queue the RSTs internally until they run out of memory.\u0026quot;\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eO-Length Headers Leak: \u0026quot;This sends a stream of headers with a 0-length header name and 0-length header value. [Node.js] allocates memory for these headers and keeps the allocation alive until the session dies. Because the names and values are 0 bytes long, the cumulative length never exceeds the header size limit.\u0026quot;\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eInternal Data Buffering: \u0026quot;This opens the HTTP/2 window so the server can send without constraint; however, it leaves the TCP window closed so the server cannot actually write (many of) the bytes on the wire. Then, the client sends a stream of requests for a large response object which the target queues internally. This appears to work to create a long-ish standing queue in node.js\u0026quot;\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eEach is a distinct issue that will need to be looked at individually. I\u0026#39;ve edited the descriptions to remove references to vulnerabilities in other HTTP/2 implementations that have not yet been disclosed.\u003c/p\u003e\n\n\u003chr\u003e\n\n\u003cp\u003eAdditional details from the report:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003e“Data Dribble” on node.js: node.js seems to queue the data internally. For a 1MB output file\nrequested 100 times in parallel fast enough that node.js is constantly processing input,\nnode.js’s RSS rises by 808MB and then falls by 120MB (for an aggregate rise of 688MB).\n(Actually, it looks like the numbers vary a bit across tests, but I think the end result is “a lot”.)\nHowever, node.js does not have the excess CPU utilization which Nginx exhibits. If you\ninstead delay the sends considerably so that node.js has time to try to send in the meantime, it\nlooks like node.js will kill off the session before the input queue grows more than a few\nhundred MB.\n\n“Internal Data Buffering” on node.js: For a 1MB output file requested 100 times in parallel\n(but sent with 24 requests per SSL frame), node.js behaves in an interesting way. It appears to\nbuffer some, but not all, data internally. It seems to continue reading (and processing requests\nand queueing data to satisfy those requests) for as many streams as it can until it can’t read\nany more. Once it can’t read anymore, it appears to try to write and realize the writing is\nblocked. At that point, it seems to switch to reading frames from the wire and queuing the\nrequests internally (without processing them). (All of this is conjecture and is based on what\nI’ve observed rather than a detailed analysis of the code.) So, if you pack the 100 requests\ninto a single SSL frame, node.js’s RSS increases by approximately 246MB. Or, if you send\n585 requests in a single SSL frame, node.js’s RSS increases by approximately 1,296MB. For\nreasons that are not entirely clear to me, if you send 100K requests each on three different\nconnections (approximately 2.8MB of request data per connection, node.js will run out of\nmemory and crash. The other interesting thing that happens is on the session ending. When\nthe session ends, it looks like node.js temporarily starts reading everything which is left in the\ninput queue, tries to process the requests, and store the request output in memory. So,\nsending 100,000 requests (approximately 2.8MB of request data) and then closing the\nconnection can make node.js temporarily use 12GB of RSS.\n\nResource Loop on node.js: Over the loopback interface, node.js can handle roughly ~10 Mb/s\nbefore the assigned thread uses 100% of its CPU core (on an m5.24xlarge). RSS rose from\n50MB at the start of the test to 236MB by the end of test (~3 minutes). RSS rose another\n156MB when a second stream was added. With two streams, serving of content to another\n(non-attacking) connection was severely impacted.\n\nZero-length Headers on node.js: With truly 0-length headers (i.e. the payload is 0 bytes), the\nserver will accept and process an unlimited number; however, they don’t seem to create a\nstanding queue on the server side. The processing overhead is much lighter than the\n“Resource Loop” test. (Roughly 25 Mb/s only produces a 75% CPU load on the server.) With\n0-length headers which are Huffman encoded into 1-byte or greater headers, the server input\nfor that socket (and only that socket) seems to get blocked for ~ 2 minutes, until the\nconnection is killed off. It appears that the server will hold the connection open even if the\nclient goes away. That behavior allows a different kind of DoS attack (exhaust server file\ndescriptors or kernel receive buffers).\n\nReset Flood on node.js: The server queue grows without an obvious bound until the\nconnection dies or the server runs out of memory and dies. After the connection ends, the\nserver is unresponsive while GC runs\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2 id=\"impact\"\u003eImpact\u003c/h2\u003e\n\n\u003cp\u003eMultiple denial of service vectors.\u003c/p\u003e\n","weakness":{"id":48,"name":"Denial of Service"},"original_report_id":null,"original_report_url":null,"attachments":[],"allow_singular_disclosure_at":"2019-09-15T15:28:56.859Z","allow_singular_disclosure_after":-40661980.12718793,"singular_disclosure_allowed":true,"vote_count":8,"voters":["bl4de","sameerphad72","dhakal_ananda","eveeez","l1nkworld","uathackerone","armansameer","salex"],"severity":{"rating":"high","author_type":"Team"},"structured_scope":{"databaseId":666,"asset_type":"SOURCE_CODE","asset_identifier":"https://github.com/nodejs/node","max_severity":"critical"},"abilities":{"assignable_team_members":[],"assignable_team_member_groups":[]},"can_edit_custom_fields_attributes":false,"activities":[{"id":4920381,"is_internal":false,"editable":false,"type":"Activities::BugTriaged","message":"","markdown_message":"","automated_response":false,"created_at":"2019-05-24T20:53:50.556Z","updated_at":"2019-05-24T20:53:50.556Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":4920383,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"I have verified the vulnerabilities here. I am in contact with the reporter and will be coordinating the communication with them.","markdown_message":"\u003cp\u003eI have verified the vulnerabilities here. I am in contact with the reporter and will be coordinating the communication with them.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-05-24T20:54:31.034Z","updated_at":"2019-05-24T20:54:31.034Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":4920440,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Thinking about possible mitigations for each of the issues...\n\n```\nData Dribble Attack: \"This program will request 1MB of data from a specified\nresource. It will request this same resource over 100 streams (so, 100MB total).\nIt manipulates window sizes and stream priority to force the server to queue\nthe data in 1-byte chunks.\"\n```\n\nWe have partial mitigation for this one already with the limits on the amount of memory a session is permitted to hold. There is, however, more that can be done. It is perfectly legitimate to have a window size of 1-byte every now and then during the course of a session but sustained 1-byte sends are a red flag. To mitigate this attack, we can implement a minimum throughput per session. If a suspiciously low throughput is sustained over multiple write operations, the session can be torn down.\n\n```\nPing Flood (nginx variant): \"Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others)\nhas a 10K-message limit on the number of control messages it will queue. Sending a controlled\nnumber of messages may enable an attacker to force the server to hold 10K messages in memory...\"\n```\n\nNode.js already implements mitigations against ping flood attacks. It would appear that this particular variant does not trigger those because of an issue within nghttp2, but I could be wrong. This one will require more investigation but we may have to wait until the fix is available in nghttp2.\n\n```\nResource Loop: \"(actually, it should be called “Priority Shuffling”): This program continually\nshuffles the priority of streams in a way which causes substantial churn to the priority tree.\nNode.js [is] particularly impacted.\"\n```\n\nThis one is going to require some thinking about. No mitigation yet.\n\n```\nReset Flood: \"This opens a number of streams and sends an invalid request over each\nstream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may\nqueue the RSTs internally until they run out of memory.\"\n```\n\nThere are two possible mitigations here:\n\n1. We try to send RST's faster without buffering them or\n2. We automatically tear down sessions that have a high number of RSTs\n\n(or both)\n\n```\nO-Length Headers Leak: \"This sends a stream of headers with a 0-length header\nname and 0-length header value. [Node.js] allocates memory for these headers\nand keeps the allocation alive until the session dies. Because the names and\nvalues are 0 bytes long, the cumulative length never exceeds the header size limit.\"\n```\n\nThis one will be fairly easy to mitigate by making sure we are not allocating memory for 0-length headers.\n\n```\nInternal Data Buffering: \"This opens the HTTP/2 window so the server can send\nwithout constraint; however, it leaves the TCP window closed so the server cannot\nactually write (many of) the bytes on the wire. Then, the client sends a stream of requests\nfor a large response object which the target queues internally. This appears to work to\ncreate a long-ish standing queue in node.js\"\n```\n\nThis one just highlights what we've already known: the internal buffering in the HTTP/2 implementation needs to be iterated on and improved to avoid the backup. Essentially, we need to implement back-pressure from the TCP socket up.","markdown_message":"\u003cp\u003eThinking about possible mitigations for each of the issues...\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eData Dribble Attack: \u0026quot;This program will request 1MB of data from a specified\nresource. It will request this same resource over 100 streams (so, 100MB total).\nIt manipulates window sizes and stream priority to force the server to queue\nthe data in 1-byte chunks.\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe have partial mitigation for this one already with the limits on the amount of memory a session is permitted to hold. There is, however, more that can be done. It is perfectly legitimate to have a window size of 1-byte every now and then during the course of a session but sustained 1-byte sends are a red flag. To mitigate this attack, we can implement a minimum throughput per session. If a suspiciously low throughput is sustained over multiple write operations, the session can be torn down.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003ePing Flood (nginx variant): \u0026quot;Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others)\nhas a 10K-message limit on the number of control messages it will queue. Sending a controlled\nnumber of messages may enable an attacker to force the server to hold 10K messages in memory...\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNode.js already implements mitigations against ping flood attacks. It would appear that this particular variant does not trigger those because of an issue within nghttp2, but I could be wrong. This one will require more investigation but we may have to wait until the fix is available in nghttp2.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eResource Loop: \u0026quot;(actually, it should be called “Priority Shuffling”): This program continually\nshuffles the priority of streams in a way which causes substantial churn to the priority tree.\nNode.js [is] particularly impacted.\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis one is going to require some thinking about. No mitigation yet.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eReset Flood: \u0026quot;This opens a number of streams and sends an invalid request over each\nstream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may\nqueue the RSTs internally until they run out of memory.\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThere are two possible mitigations here:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eWe try to send RST\u0026#39;s faster without buffering them or\u003c/li\u003e\n\u003cli\u003eWe automatically tear down sessions that have a high number of RSTs\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e(or both)\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eO-Length Headers Leak: \u0026quot;This sends a stream of headers with a 0-length header\nname and 0-length header value. [Node.js] allocates memory for these headers\nand keeps the allocation alive until the session dies. Because the names and\nvalues are 0 bytes long, the cumulative length never exceeds the header size limit.\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis one will be fairly easy to mitigate by making sure we are not allocating memory for 0-length headers.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eInternal Data Buffering: \u0026quot;This opens the HTTP/2 window so the server can send\nwithout constraint; however, it leaves the TCP window closed so the server cannot\nactually write (many of) the bytes on the wire. Then, the client sends a stream of requests\nfor a large response object which the target queues internally. This appears to work to\ncreate a long-ish standing queue in node.js\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis one just highlights what we\u0026#39;ve already known: the internal buffering in the HTTP/2 implementation needs to be iterated on and improved to avoid the backup. Essentially, we need to implement back-pressure from the TCP socket up.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-05-24T21:13:54.450Z","updated_at":"2019-05-24T21:13:54.450Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":4923075,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"How do we plan to work on those? James, would you be able to tackle them or do you need some help? Have you got some repro scripts that we can use?\n\nAre you in contact with the nghttp2 author regarding the disclosure \u0026 fix of that? I think it would be good to coordinate releases.","markdown_message":"\u003cp\u003eHow do we plan to work on those? James, would you be able to tackle them or do you need some help? Have you got some repro scripts that we can use?\u003c/p\u003e\n\n\u003cp\u003eAre you in contact with the nghttp2 author regarding the disclosure \u0026amp; fix of that? I think it would be good to coordinate releases.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-05-25T13:36:53.694Z","updated_at":"2019-05-25T13:36:53.694Z","actor":{"username":"mcollina","cleared":false,"url":"/mcollina","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/230/980/61a26e6fa4ec002fca494b3b7b43aa251eef3453_original.jpeg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":4923143,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Help is definitely appreciated. I'll do what I can tho.\n\nI am in contact, yes. Coordinating the release is going to be complicated because the report was made to *many* implementations and the reporter is asking that none release before the others. To give you an idea of the scope, this impacts go, nginx, Apache, and others.","markdown_message":"\u003cp\u003eHelp is definitely appreciated. I\u0026#39;ll do what I can tho.\u003c/p\u003e\n\n\u003cp\u003eI am in contact, yes. Coordinating the release is going to be complicated because the report was made to \u003cem\u003emany\u003c/em\u003e implementations and the reporter is asking that none release before the others. To give you an idea of the scope, this impacts go, nginx, Apache, and others.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-05-25T14:29:24.931Z","updated_at":"2019-05-25T14:29:24.931Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5138925,"is_internal":false,"editable":false,"type":"Activities::NotEligibleForBounty","message":"As per our policy this is not eligible for a bounty.","markdown_message":"\u003cp\u003eAs per our policy this is not eligible for a bounty.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-06-19T18:17:06.294Z","updated_at":"2019-06-19T18:17:06.294Z","actor":{"url":"/nodejs","ibb":true,"profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/022/984/e600648ace4a8553247bce967d461a030aa81d49_original.png/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"profile":{"name":"Node.js"}},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5282801,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Just a reminder, this still needs to be worked on. I've been on PTO and otherwise largely unavailable. I wont be able to look at this in detail until next week. It would be great to have others helping so we can get these issues fixed before disclosure","markdown_message":"\u003cp\u003eJust a reminder, this still needs to be worked on. I\u0026#39;ve been on PTO and otherwise largely unavailable. I wont be able to look at this in detail until next week. It would be great to have others helping so we can get these issues fixed before disclosure\u003c/p\u003e\n","automated_response":false,"created_at":"2019-07-09T16:11:29.669Z","updated_at":"2019-07-09T16:11:29.669Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5303048,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Is there enough information here for someone else to work on it? If someone steps up to work on it, can we put them in touch with the reporter or something in case they have questions or want exploit code so they can test their fixes etc.?\n\nThe zero-length header one seems like there's enough info already here for sure. But I'm less certain about the other ones.","markdown_message":"\u003cp\u003eIs there enough information here for someone else to work on it? If someone steps up to work on it, can we put them in touch with the reporter or something in case they have questions or want exploit code so they can test their fixes etc.?\u003c/p\u003e\n\n\u003cp\u003eThe zero-length header one seems like there\u0026#39;s enough info already here for sure. But I\u0026#39;m less certain about the other ones.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-07-11T22:40:27.484Z","updated_at":"2019-07-11T22:40:27.484Z","actor":{"username":"trott","cleared":false,"url":"/trott","profile_picture_urls":{"medium":"/assets/avatars/default-71a302d706457f3d3a31eb30fa3e73e6cf0b1d677b8fa218eaeaffd67ae97918.png"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5303066,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Hm yeah, I’ll assume that I’m one of the best people to look at this while James is unavailable (and tbh he also asked me to do that a while ago), so I’ll make this a priority for me and spend time on this tomorrow and on the weekend.\n\nThat being said – a lot of these revolve around low-level details of the protocol that I don’t fully grasp (yet). I do agree that the zero-length header one is well-defined enough to write a (simple) patch to address it.","markdown_message":"\u003cp\u003eHm yeah, I’ll assume that I’m one of the best people to look at this while James is unavailable (and tbh he also asked me to do that a while ago), so I’ll make this a priority for me and spend time on this tomorrow and on the weekend.\u003c/p\u003e\n\n\u003cp\u003eThat being said – a lot of these revolve around low-level details of the protocol that I don’t fully grasp (yet). I do agree that the zero-length header one is well-defined enough to write a (simple) patch to address it.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-07-11T22:51:06.558Z","updated_at":"2019-07-11T22:51:06.558Z","actor":{"username":"addaleax","cleared":false,"url":"/addaleax","profile_picture_urls":{"medium":"/assets/avatars/default-71a302d706457f3d3a31eb30fa3e73e6cf0b1d677b8fa218eaeaffd67ae97918.png"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5319578,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"So far:\n\n1. For most of these issues, it would be very very helpful to have reproduction code to work and test with. I haven’t done that yet, and it might not be easy to get this done with Node itself because of how much state nghttp2 manages for us.\n2. A lot of this boils down to our memory tracking code not being very good. Improving both the way in which we track allocations and making it close to accurate, and improving the way with which we react to exceeding that limit, is probably the best solution here. This is not going to be trivial, esp. when it comes to making guesses about how it’s going to impact our users.","markdown_message":"\u003cp\u003eSo far:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eFor most of these issues, it would be very very helpful to have reproduction code to work and test with. I haven’t done that yet, and it might not be easy to get this done with Node itself because of how much state nghttp2 manages for us.\u003c/li\u003e\n\u003cli\u003eA lot of this boils down to our memory tracking code not being very good. Improving both the way in which we track allocations and making it close to accurate, and improving the way with which we react to exceeding that limit, is probably the best solution here. This is not going to be trivial, esp. when it comes to making guesses about how it’s going to impact our users.\u003c/li\u003e\n\u003c/ol\u003e\n","automated_response":false,"created_at":"2019-07-14T23:26:17.012Z","updated_at":"2019-07-14T23:26:17.012Z","actor":{"username":"addaleax","cleared":false,"url":"/addaleax","profile_picture_urls":{"medium":"/assets/avatars/default-71a302d706457f3d3a31eb30fa3e73e6cf0b1d677b8fa218eaeaffd67ae97918.png"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5471830,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"@addaleax ... if you have keybase, I can add you the keybase chat for the larger vulnerability discussion","markdown_message":"\u003cp\u003e\u003ca href=\"/addaleax\"\u003e@addaleax\u003c/a\u003e ... if you have keybase, I can add you the keybase chat for the larger vulnerability discussion\u003c/p\u003e\n","automated_response":false,"created_at":"2019-07-31T16:29:48.179Z","updated_at":"2019-07-31T16:29:48.179Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5471943,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"@jasnell I do, same handle as everywhere and just followed you there. :)","markdown_message":"\u003cp\u003e\u003ca href=\"/jasnell\"\u003e@jasnell\u003c/a\u003e I do, same handle as everywhere and just followed you there. :)\u003c/p\u003e\n","automated_response":false,"created_at":"2019-07-31T16:48:59.301Z","updated_at":"2019-07-31T16:48:59.301Z","actor":{"username":"addaleax","cleared":false,"url":"/addaleax","profile_picture_urls":{"medium":"/assets/avatars/default-71a302d706457f3d3a31eb30fa3e73e6cf0b1d677b8fa218eaeaffd67ae97918.png"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5520420,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"So, to give an update: I’m unfortunately lagging behind a bit with working on this. I’ve looked into all of the issues at least to some degree, but I won’t have much time until Friday to start working on actual patches, and I assume we’d want to discuss them in review before releasing them.\n\nThat makes the approaching current disclosure date of August 13th (Tuesday), which I don’t think has been mentioned here before, a bit close. I don’t think we’ve prepared anything in terms of security releases, so I’m not sure how that is going to go? Do we want that to be our own deadline?\n\nI also unfortunately won’t be able to join the TSC meeting today.","markdown_message":"\u003cp\u003eSo, to give an update: I’m unfortunately lagging behind a bit with working on this. I’ve looked into all of the issues at least to some degree, but I won’t have much time until Friday to start working on actual patches, and I assume we’d want to discuss them in review before releasing them.\u003c/p\u003e\n\n\u003cp\u003eThat makes the approaching current disclosure date of August 13th (Tuesday), which I don’t think has been mentioned here before, a bit close. I don’t think we’ve prepared anything in terms of security releases, so I’m not sure how that is going to go? Do we want that to be our own deadline?\u003c/p\u003e\n\n\u003cp\u003eI also unfortunately won’t be able to join the TSC meeting today.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-08-07T14:56:05.508Z","updated_at":"2019-08-07T14:56:05.508Z","actor":{"username":"addaleax","cleared":false,"url":"/addaleax","profile_picture_urls":{"medium":"/assets/avatars/default-71a302d706457f3d3a31eb30fa3e73e6cf0b1d677b8fa218eaeaffd67ae97918.png"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5546505,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"\u003e If we ask for more time, it means delaying planned security releases in a number of other projects. That may be a hard sell, but I suppose we could ask. (James would have to say for sure, I think, as he's the one who has been in touch with the coordinators here. \n\nYeah, I think that’s accurate.\n\n\u003e At least, I don't think anyone else on the Node.js side has.)\n\nI have been added to the keybase chat mentioned above ~10 days ago. I don’t know who the reporter/person “in charge” of the deadline is, though.","markdown_message":"\u003cblockquote\u003e\n\u003cp\u003eIf we ask for more time, it means delaying planned security releases in a number of other projects. That may be a hard sell, but I suppose we could ask. (James would have to say for sure, I think, as he\u0026#39;s the one who has been in touch with the coordinators here. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eYeah, I think that’s accurate.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAt least, I don\u0026#39;t think anyone else on the Node.js side has.)\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eI have been added to the keybase chat mentioned above ~10 days ago. I don’t know who the reporter/person “in charge” of the deadline is, though.\u003c/p\u003e\n","automated_response":false,"created_at":"2019-08-10T23:16:33.592Z","updated_at":"2019-08-10T23:16:33.592Z","actor":{"username":"addaleax","cleared":false,"url":"/addaleax","profile_picture_urls":{"medium":"/assets/avatars/default-71a302d706457f3d3a31eb30fa3e73e6cf0b1d677b8fa218eaeaffd67ae97918.png"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5587409,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Fixes for this have been released for almost all platforms (Linux ARMv6 for 8.x is still building): https://nodejs.org/en/blog/vulnerability/aug-2019-security-releases/","markdown_message":"\u003cp\u003eFixes for this have been released for almost all platforms (Linux ARMv6 for 8.x is still building): \u003ca title=\"https://nodejs.org/en/blog/vulnerability/aug-2019-security-releases/\" href=\"/redirect?url=https%3A%2F%2Fnodejs.org%2Fen%2Fblog%2Fvulnerability%2Faug-2019-security-releases%2F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003e\u003cspan\u003ehttps://nodejs.org/en/blog/vulnerability/aug-2019-security-releases/\u003c/span\u003e\u003ci class=\"icon-external-link\"\u003e\u003c/i\u003e\u003c/a\u003e\u003c/p\u003e\n","automated_response":false,"created_at":"2019-08-15T23:13:05.569Z","updated_at":"2019-08-15T23:13:05.569Z","actor":{"username":"octetcloud","cleared":false,"url":"/octetcloud","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/210/672/27b2c0c2193cfb366f7889374e7ce71077431f74_original.jpeg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5589605,"is_internal":false,"editable":false,"type":"Activities::Comment","message":"Yes, this can be closed and made public. ","markdown_message":"\u003cp\u003eYes, this can be closed and made public. \u003c/p\u003e\n","automated_response":false,"created_at":"2019-08-16T09:02:53.008Z","updated_at":"2019-08-16T09:02:53.008Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5591892,"is_internal":false,"editable":false,"type":"Activities::ReportSeverityUpdated","message":"","markdown_message":"","automated_response":false,"created_at":"2019-08-16T15:26:59.725Z","updated_at":"2019-08-16T15:26:59.725Z","additional_data":{"old_severity":null,"new_severity":"High","old_severity_id":null,"new_severity_id":494391},"actor":{"username":"octetcloud","cleared":false,"url":"/octetcloud","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/210/672/27b2c0c2193cfb366f7889374e7ce71077431f74_original.jpeg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5591895,"is_internal":false,"editable":false,"type":"Activities::BugResolved","message":"","markdown_message":"","automated_response":false,"created_at":"2019-08-16T15:27:11.944Z","updated_at":"2019-08-16T15:27:11.944Z","actor":{"username":"octetcloud","cleared":false,"url":"/octetcloud","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/210/672/27b2c0c2193cfb366f7889374e7ce71077431f74_original.jpeg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"reporter":{"username":"jasnell","url":"/jasnell"},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5591905,"is_internal":false,"editable":false,"type":"Activities::AgreedOnGoingPublic","message":"","markdown_message":"","automated_response":false,"created_at":"2019-08-16T15:28:56.825Z","updated_at":"2019-08-16T15:28:56.825Z","first_to_agree":true,"actor":{"username":"octetcloud","cleared":false,"url":"/octetcloud","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/210/672/27b2c0c2193cfb366f7889374e7ce71077431f74_original.jpeg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":false,"actor_is_concealed_member":false},{"id":5594938,"is_internal":false,"editable":false,"type":"Activities::AgreedOnGoingPublic","message":"","markdown_message":"","automated_response":false,"created_at":"2019-08-16T23:40:09.444Z","updated_at":"2019-08-16T23:40:09.444Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false},{"id":5594939,"is_internal":false,"editable":false,"type":"Activities::ReportBecamePublic","message":"","markdown_message":"","automated_response":false,"created_at":"2019-08-16T23:40:09.503Z","updated_at":"2019-08-16T23:40:09.503Z","actor":{"username":"jasnell","cleared":false,"url":"/jasnell","profile_picture_urls":{"medium":"https://profile-photos.hackerone-user-content.com/variants/000/211/162/109187a55a3f81113d4b61b965ec72cbaa4970e3_original.jpg/eb31823a4cc9f6b6bb4db930ffdf512533928a68a4255fb50a83180281a60da5"},"hackerone_triager":false,"hackerone_employee":null},"genius_execution_id":null,"team_handle":"nodejs","actor_is_team_member":true,"actor_is_concealed_member":false}],"activity_page_count":1,"activity_page_number":1,"summaries":[{"id":17882,"category":"team","content":"A security researcher conducted a broad survey of HTTP/2 implementations to investigate common Denial of Service attack vectors. The Node.js implementation was been found to be subject to a number of these issues.\n\nSpecifically:\n\nData Dribble Attack: \"This program will request 1MB of data from a specified resource. It will request this same resource over 100 streams (so, 100MB total). It manipulates window sizes and stream priority to force the server to queue the data in 1-byte chunks.\"\n\nPing Flood (nginx variant): \"Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others) has a 10K-message limit on the number of control messages it will queue. Sending a controlled number of messages may enable an attacker to force the server to hold 10K messages in memory...\"\n\nResource Loop: \"(actually, it should be called “Priority Shuffling”): This program continually shuffles the priority of streams in a way which causes substantial churn to the priority tree. Node.js [is] particularly impacted.\"\n\nReset Flood: \"This opens a number of streams and sends an invalid request over each stream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may queue the RSTs internally until they run out of memory.\"\n\nO-Length Headers Leak: \"This sends a stream of headers with a 0-length header name and 0-length header value. [Node.js] allocates memory for these headers and keeps the allocation alive until the session dies. Because the names and values are 0 bytes long, the cumulative length never exceeds the header size limit.\"\n\nInternal Data Buffering: \"This opens the HTTP/2 window so the server can send without constraint; however, it leaves the TCP window closed so the server cannot actually write (many of) the bytes on the wire. Then, the client sends a stream of requests for a large response object which the target queues internally. This appears to work to create a long-ish standing queue in node.js\"\n\nEach is a distinct issue that will need to be looked at individually. I've edited the descriptions to remove references to vulnerabilities in other HTTP/2 implementations that have not yet been disclosed.\n\nAdditional details from the report:\n\n“Data Dribble” on node.js: node.js seems to queue the data internally. For a 1MB output file\nrequested 100 times in parallel fast enough that node.js is constantly processing input,\nnode.js’s RSS rises by 808MB and then falls by 120MB (for an aggregate rise of 688MB).\n(Actually, it looks like the numbers vary a bit across tests, but I think the end result is “a lot”.)\nHowever, node.js does not have the excess CPU utilization which Nginx exhibits. If you\ninstead delay the sends considerably so that node.js has time to try to send in the meantime, it\nlooks like node.js will kill off the session before the input queue grows more than a few\nhundred MB.\n\n“Internal Data Buffering” on node.js: For a 1MB output file requested 100 times in parallel\n(but sent with 24 requests per SSL frame), node.js behaves in an interesting way. It appears to\nbuffer some, but not all, data internally. It seems to continue reading (and processing requests\nand queueing data to satisfy those requests) for as many streams as it can until it can’t read\nany more. Once it can’t read anymore, it appears to try to write and realize the writing is\nblocked. At that point, it seems to switch to reading frames from the wire and queuing the\nrequests internally (without processing them). (All of this is conjecture and is based on what\nI’ve observed rather than a detailed analysis of the code.) So, if you pack the 100 requests\ninto a single SSL frame, node.js’s RSS increases by approximately 246MB. Or, if you send\n585 requests in a single SSL frame, node.js’s RSS increases by approximately 1,296MB. For\nreasons that are not entirely clear to me, if you send 100K requests each on three different\nconnections (approximately 2.8MB of request data per connection, node.js will run out of\nmemory and crash. The other interesting thing that happens is on the session ending. When\nthe session ends, it looks like node.js temporarily starts reading everything which is left in the\ninput queue, tries to process the requests, and store the request output in memory. So,\nsending 100,000 requests (approximately 2.8MB of request data) and then closing the\nconnection can make node.js temporarily use 12GB of RSS.\n\nResource Loop on node.js: Over the loopback interface, node.js can handle roughly ~10 Mb/s\nbefore the assigned thread uses 100% of its CPU core (on an m5.24xlarge). RSS rose from\n50MB at the start of the test to 236MB by the end of test (~3 minutes). RSS rose another\n156MB when a second stream was added. With two streams, serving of content to another\n(non-attacking) connection was severely impacted.\n\nZero-length Headers on node.js: With truly 0-length headers (i.e. the payload is 0 bytes), the\nserver will accept and process an unlimited number; however, they don’t seem to create a\nstanding queue on the server side. The processing overhead is much lighter than the\n“Resource Loop” test. (Roughly 25 Mb/s only produces a 75% CPU load on the server.) With\n0-length headers which are Huffman encoded into 1-byte or greater headers, the server input\nfor that socket (and only that socket) seems to get blocked for ~ 2 minutes, until the\nconnection is killed off. It appears that the server will hold the connection open even if the\nclient goes away. That behavior allows a different kind of DoS attack (exhaust server file\ndescriptors or kernel receive buffers).\n\nReset Flood on node.js: The server queue grows without an obvious bound until the\nconnection dies or the server runs out of memory and dies. After the connection ends, the\nserver is unresponsive while GC runs\n\nImpact:\n\nMultiple denial of service vectors.","can_view?":true,"can_edit?":false,"content_html":"\u003cp\u003eA security researcher conducted a broad survey of HTTP/2 implementations to investigate common Denial of Service attack vectors. The Node.js implementation was been found to be subject to a number of these issues.\u003c/p\u003e\n\n\u003cp\u003eSpecifically:\u003c/p\u003e\n\n\u003cp\u003eData Dribble Attack: \u0026quot;This program will request 1MB of data from a specified resource. It will request this same resource over 100 streams (so, 100MB total). It manipulates window sizes and stream priority to force the server to queue the data in 1-byte chunks.\u0026quot;\u003c/p\u003e\n\n\u003cp\u003ePing Flood (nginx variant): \u0026quot;Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others) has a 10K-message limit on the number of control messages it will queue. Sending a controlled number of messages may enable an attacker to force the server to hold 10K messages in memory...\u0026quot;\u003c/p\u003e\n\n\u003cp\u003eResource Loop: \u0026quot;(actually, it should be called “Priority Shuffling”): This program continually shuffles the priority of streams in a way which causes substantial churn to the priority tree. Node.js [is] particularly impacted.\u0026quot;\u003c/p\u003e\n\n\u003cp\u003eReset Flood: \u0026quot;This opens a number of streams and sends an invalid request over each stream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may queue the RSTs internally until they run out of memory.\u0026quot;\u003c/p\u003e\n\n\u003cp\u003eO-Length Headers Leak: \u0026quot;This sends a stream of headers with a 0-length header name and 0-length header value. [Node.js] allocates memory for these headers and keeps the allocation alive until the session dies. Because the names and values are 0 bytes long, the cumulative length never exceeds the header size limit.\u0026quot;\u003c/p\u003e\n\n\u003cp\u003eInternal Data Buffering: \u0026quot;This opens the HTTP/2 window so the server can send without constraint; however, it leaves the TCP window closed so the server cannot actually write (many of) the bytes on the wire. Then, the client sends a stream of requests for a large response object which the target queues internally. This appears to work to create a long-ish standing queue in node.js\u0026quot;\u003c/p\u003e\n\n\u003cp\u003eEach is a distinct issue that will need to be looked at individually. I\u0026#39;ve edited the descriptions to remove references to vulnerabilities in other HTTP/2 implementations that have not yet been disclosed.\u003c/p\u003e\n\n\u003cp\u003eAdditional details from the report:\u003c/p\u003e\n\n\u003cp\u003e“Data Dribble” on node.js: node.js seems to queue the data internally. For a 1MB output file\u003cbr\u003e\nrequested 100 times in parallel fast enough that node.js is constantly processing input,\u003cbr\u003e\nnode.js’s RSS rises by 808MB and then falls by 120MB (for an aggregate rise of 688MB).\u003cbr\u003e\n(Actually, it looks like the numbers vary a bit across tests, but I think the end result is “a lot”.)\u003cbr\u003e\nHowever, node.js does not have the excess CPU utilization which Nginx exhibits. If you\u003cbr\u003e\ninstead delay the sends considerably so that node.js has time to try to send in the meantime, it\u003cbr\u003e\nlooks like node.js will kill off the session before the input queue grows more than a few\u003cbr\u003e\nhundred MB.\u003c/p\u003e\n\n\u003cp\u003e“Internal Data Buffering” on node.js: For a 1MB output file requested 100 times in parallel\u003cbr\u003e\n(but sent with 24 requests per SSL frame), node.js behaves in an interesting way. It appears to\u003cbr\u003e\nbuffer some, but not all, data internally. It seems to continue reading (and processing requests\u003cbr\u003e\nand queueing data to satisfy those requests) for as many streams as it can until it can’t read\u003cbr\u003e\nany more. Once it can’t read anymore, it appears to try to write and realize the writing is\u003cbr\u003e\nblocked. At that point, it seems to switch to reading frames from the wire and queuing the\u003cbr\u003e\nrequests internally (without processing them). (All of this is conjecture and is based on what\u003cbr\u003e\nI’ve observed rather than a detailed analysis of the code.) So, if you pack the 100 requests\u003cbr\u003e\ninto a single SSL frame, node.js’s RSS increases by approximately 246MB. Or, if you send\u003cbr\u003e\n585 requests in a single SSL frame, node.js’s RSS increases by approximately 1,296MB. For\u003cbr\u003e\nreasons that are not entirely clear to me, if you send 100K requests each on three different\u003cbr\u003e\nconnections (approximately 2.8MB of request data per connection, node.js will run out of\u003cbr\u003e\nmemory and crash. The other interesting thing that happens is on the session ending. When\u003cbr\u003e\nthe session ends, it looks like node.js temporarily starts reading everything which is left in the\u003cbr\u003e\ninput queue, tries to process the requests, and store the request output in memory. So,\u003cbr\u003e\nsending 100,000 requests (approximately 2.8MB of request data) and then closing the\u003cbr\u003e\nconnection can make node.js temporarily use 12GB of RSS.\u003c/p\u003e\n\n\u003cp\u003eResource Loop on node.js: Over the loopback interface, node.js can handle roughly ~10 Mb/s\u003cbr\u003e\nbefore the assigned thread uses 100% of its CPU core (on an m5.24xlarge). RSS rose from\u003cbr\u003e\n50MB at the start of the test to 236MB by the end of test (~3 minutes). RSS rose another\u003cbr\u003e\n156MB when a second stream was added. With two streams, serving of content to another\u003cbr\u003e\n(non-attacking) connection was severely impacted.\u003c/p\u003e\n\n\u003cp\u003eZero-length Headers on node.js: With truly 0-length headers (i.e. the payload is 0 bytes), the\u003cbr\u003e\nserver will accept and process an unlimited number; however, they don’t seem to create a\u003cbr\u003e\nstanding queue on the server side. The processing overhead is much lighter than the\u003cbr\u003e\n“Resource Loop” test. (Roughly 25 Mb/s only produces a 75% CPU load on the server.) With\u003cbr\u003e\n0-length headers which are Huffman encoded into 1-byte or greater headers, the server input\u003cbr\u003e\nfor that socket (and only that socket) seems to get blocked for ~ 2 minutes, until the\u003cbr\u003e\nconnection is killed off. It appears that the server will hold the connection open even if the\u003cbr\u003e\nclient goes away. That behavior allows a different kind of DoS attack (exhaust server file\u003cbr\u003e\ndescriptors or kernel receive buffers).\u003c/p\u003e\n\n\u003cp\u003eReset Flood on node.js: The server queue grows without an obvious bound until the\u003cbr\u003e\nconnection dies or the server runs out of memory and dies. After the connection ends, the\u003cbr\u003e\nserver is unresponsive while GC runs\u003c/p\u003e\n\n\u003cp\u003eImpact:\u003c/p\u003e\n\n\u003cp\u003eMultiple denial of service vectors.\u003c/p\u003e\n"},{"category":"researcher","can_view?":true,"can_create?":false}]}